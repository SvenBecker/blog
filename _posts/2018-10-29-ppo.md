---
title: Proximal Policy Optimization
date: 2018-10-23
---

**Notice!** This page is still in progress!
{: .notice--primary}

# Proximal Policy Optimization

*Proximal Policy Optimization* (PPO) is a policy gradient algorithm developed by [Schulman et al. (2017)](https://arxiv.org/abs/1707.06347).
It is closely related to the *Trust Region Policy Optimization* (TRPO) by [Schulman et al. (2015)](https://arxiv.org/abs/1502.05477) but
empirically achieves faster convergence times and offers smaller computation costs.
Before diving deep into PPO I will assume you are familiar with basic reinforcement learning and policy gradient algorithms.
Lets start by giving an introduction on TRPO because this is the fundamanetal basis of the PPO algorithm.

## Trust Region Policy Optimization

### TRPO Objective Function

## PPO Transition

### PPO Objective Function

## References

- __Paper__ *Schulman et al.*: [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477), 2015
- __Paper__ *Schulman et al.*: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347), 2017
- __Blog Post__ *Jonathan Hui*: [RL — Proximal Policy Optimization (PPO) Explained](https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)
- __Blog Post__ *Jonathan Hui*: [RL — Trust Region Policy Optimization (TRPO) Explained](https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9)
- __Blog Post__ *Jonathan Hui*: [RL — Trust Region Policy Optimization (TRPO) Part 2](https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a)