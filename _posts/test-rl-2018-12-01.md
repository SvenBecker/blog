---
title: Reinforcement Learning - An Introduction
date: 2018-10-23
header:
    overlay_image: https://deepmind.com/static/v0.0.0/images/deepmind_logo.png
    teaser: https://images.unsplash.com/photo-1485827404703-89b55fcc595e?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=139f00301feb37e712adda8bf9d8b91f&auto=format&fit=crop&w=1500&q=80
    og_image: https://deepmind.com/static/v0.0.0/images/deepmind_logo.png
    overlay_filter: 0.7
    caption: "Image credit: [**DeepMind**](deepmind.com)"
excerpt: "A short Introduction to core concepts of Reinforcement-Learning"
categories:
    - Deep Learning
    - Academic
tags:
    - Reinforcement Learning
---

# Reinforcement Learning - An Introduction

![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)

## Dynamic Programming

Model-based vs. Model-free

## Markov Decision Process

![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/400px-Markov_Decision_Process.svg.png)

## Exploration vs. Exploitation

*Exploration* versus *Exploitation* is a core problem in RL. What we want to achieve in RL is to train an agent who is capable of finding good, if not even the best, actions given a certain numerical representation of the environment. But as we learned already the agent has an influence on the very same environment he later observes.

In a very small discrete environment it might be possible to just try every discrete action on every single possible resulting state and select the optimal policy greedily. This would be a dynamic programming approach.

But most of the time this approach is due to a continous state and/or action space infeasible and even in discrete state and action spaces this approach might be too computational expensive.

Thus we have to *explore* the state space given our action space to find a satisfying policy. This might be done by just randomly selecting random actions to explore a wide range of possbile state spaces and resulting rewards.

But on the other site we might never converge to an optimal policy because we are only exploring and not actually selecting the best actions given the current state representation. The selection of the best action which has been found so far, is being called *Exploitation*.

So what we want to do is to find a middle route between this two concepts.
Most of the time in academia we follow an exploitation approach, select the best action, but also include some sort of randomness to the selection of the action to achieve a state space exploration.

![](https://www.oreilly.com/library/view/the-innovation-book/9781292011905/images/f0227_01.png)

### $\epsilon$-Greedy

The most straight forward way is the so called $\epsilon$-greedy approach. Here $\epsilon$ denotes a probability for selecting the best action found so far for the given state observation.
We will select this action with the probability $1 - \epsilon$, where $0 \leq \epsilon \leq 1$, and with the probability $\epsilon$ we will select a different random action from our action space. Thus we either get $\pi(s_t) = \text{argmax}_{a \in A(s_t)}Q(s_t,a)$ or $\pi(s_t) \sim u(A(s_t))$, assuming the random actions are uniform distributed.

**Python/Numpy Implementation**

```python
e = 0.1

# random initialization of Q-table
q_values = {s: np.random.rand(len(action_space)) for s in state_space}
x = np.random.rand(1)

if x < e:
    # assuming we want the 
    a = np.argmax(np.array(q_values[0]))
else:
    a = np.random.choice(action_space)
```

### Bolzmann Approach

The Bolzman approach, also often called the softmax approach, has the purpose to provide a probability distribution of action values given the current state.
$$
\pi(s_t, a) = \frac{\exp(\frac{Q(s_t,a)}{T})}{\sum_{a \in A(s_t)} \exp(\frac{Q(s_t,a)}{T})}
$$
$\pi(s_t, a_t)$ denotes the propability of choosing action $a_t$ in $s_t$. $T \geq 0$ is the so called temperature parameter of the Bolzmann distribution. If $T=0$ the agent does not explore the state space at all whereas $T \rightarrow \infty$ will result in only random actions. This will give us control of the exploration-exploitation tradeoff just be varying $T$.

```python
# random initialization of Q-table
q_values = {s: np.random.rand(len(action_space)) for s in state_space}

T = 0.1

action_probs = np.exp(np.argmax(q_values[0]) / T) / np.sum(np.exp(q_values[0]/ T))
```

### Minimizing Regret

Another approach is to minimize the regret, which is basically equivalent to maximizing the cumulative reward but allows a better distinction between the linear and sublinear reward case. Lets denote the regret we achieve at timestep $t$ as $L_t$, we can formulate the regret as 
$$L_t = \sum_{i=1}^{t} (v_{*} - q(a_i)).$$
Based on this regret formulation there are multiple ways to obtain exploration where currently the *UCB* algorithm is probably the most common one.

## Function Approximation

## Value Based vs. Policy Based